# -*- coding: utf-8 -*-
"""BERT_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/101pVkP0ig6l-eTOyRzB9YGFvVI7tZZxq
"""

!pip install --upgrade pip

!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117

!pip install transformers datasets evaluate accelerate sentencepiece scikit-learn tqdm huggingface_hub

!pip install tensorflow transformers datasets evaluate scikit-learn tqdm huggingface_hub

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("GPU available:", tf.config.list_physical_devices("GPU"))

from sklearn.model_selection import train_test_split
from datasets import load_dataset
dataset = load_dataset("imdb")
train_valid = dataset["train"].shuffle(seed=42).train_test_split(test_size=0.1)
train_dataset = train_valid["train"]
valid_dataset = train_valid["test"]
test_dataset = dataset["test"]

from transformers import AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize, batched=True)

from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")

train_dataset = tokenized_dataset["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

eval_dataset = tokenized_dataset["test"].to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)

from transformers import AutoModelForSequenceClassification

num_labels = len(set(dataset["train"]["label"]))
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW  # ðŸ‘ˆ fix here
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset

# Load dataset
dataset = load_dataset("imdb")
train_dataset = dataset["train"]
test_dataset = dataset["test"]

# Load tokenizer and model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize
def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Convert to PyTorch tensors
train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

# Optimizer & loss
optimizer = AdamW(model.parameters(), lr=5e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import os

save_dir = "fine_tuned_imdb"
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("Saved files:", os.listdir(save_dir))

!zip -r fine_tuned_imdb.zip fine_tuned_imdb
from google.colab import files
files.download("fine_tuned_imdb.zip")

pip install huggingface_hub

from huggingface_hub import notebook_login
notebook_login()

from transformers import pipeline

sentiment_pipeline = pipeline("text-classification", model="your-username/fine-tuned-imdb")

print(sentiment_pipeline("I loved this movie!"))
print(sentiment_pipeline("This was awful."))